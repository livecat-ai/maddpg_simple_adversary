{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + '/opt/X11/bin'\n",
    "# import envs\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "\n",
    "from pettingzoo.mpe import simple_adversary_v2\n",
    "\n",
    "# keep training awake\n",
    "from workspace_utils import keep_awake\n",
    "\n",
    "# for saving gif\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    seeding()\n",
    "    # number of parallel agents\n",
    "    parallel_envs = 1\n",
    "    # number of training episodes.\n",
    "    # change this to higher number to experiment. say 30000.\n",
    "    number_of_episodes = 1500\n",
    "    episode_length = 25\n",
    "    batchsize = 1024\n",
    "    # how many episodes to save policy and gif\n",
    "    save_interval = 50\n",
    "    # t = 0\n",
    "    \n",
    "    # amplitude of OU noise\n",
    "    # this slowly decreases to 0\n",
    "    noise = 2\n",
    "    noise_reduction = 0.999\n",
    "\n",
    "    # how many episodes before update\n",
    "    episode_per_update = 1\n",
    "\n",
    "    log_path = os.getcwd()+\"/log\"\n",
    "    model_dir= os.getcwd()+\"/model_dir\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    env = simple_adversary_v2.env(continuous_actions=True, render_mode='rgb_array', max_cycles=episode_length)\n",
    "     \n",
    "    # keep 5000 episodes worth of replay\n",
    "    buffer = ReplayBuffer(int(5000*episode_length))\n",
    "    \n",
    "    # initialize policy and critic\n",
    "    maddpg = MADDPG()\n",
    "    logger = SummaryWriter(log_dir=log_path)\n",
    "    # agent0_reward = []\n",
    "    # agent1_reward = []\n",
    "    # agent2_reward = []\n",
    "\n",
    "    # training loop\n",
    "    # show progressbar\n",
    "    import progressbar as pb\n",
    "    widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "              pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "    \n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "\n",
    "    # use keep_awake to keep workspace from disconnecting\n",
    "    for episode in range(0, number_of_episodes):\n",
    "\n",
    "        timer.update(episode)\n",
    "\n",
    "        reward_this_episode = np.zeros(3)\n",
    "        env.reset() #\n",
    "        agents = env.agents\n",
    "        num_agents = env.num_agents\n",
    "\n",
    "        dummy_action = torch.tensor(np.zeros(5,))\n",
    "        obs = [env.observe(agent) for agent in agents]\n",
    "       \n",
    "        for _ in range(num_agents):\n",
    "            env.step(dummy_action)\n",
    "\n",
    "        #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "        # save info or not\n",
    "        save_info = (episode) % save_interval == 0\n",
    "        frames = []\n",
    "        tmax = 0\n",
    "        \n",
    "        # if save_info:\n",
    "        #     frames.append(env.render('rgb_array'))\n",
    "        \n",
    "        for episode_t in range(episode_length):\n",
    "            \n",
    "            # explore = only explore for a certain number of episodes\n",
    "            # action input needs to be transposed\n",
    "            obs_tensor = [torch.tensor(ob, dtype=torch.float32) for ob in obs]\n",
    "            actions = maddpg.act(obs_tensor, noise=noise)\n",
    "            actions = [action.clip(0.0, 1.0).detach().numpy() for action in actions]\n",
    "\n",
    "            noise *= noise_reduction\n",
    "            \n",
    "            # step forward one frame\n",
    "            next_obs, rewards, dones, infos = [], [], [], []\n",
    "            for i in range(num_agents): \n",
    "                next_ob, reward, termination, truncation, info = env.last()\n",
    "                done = termination or truncation\n",
    "                if done:\n",
    "                    break\n",
    "                next_obs.append(next_ob)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                infos.append(info)\n",
    "                env.step(actions[i])\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            # print(len(actions), len(actions[0]))\n",
    "            # raise\n",
    "            transition = (obs, actions, rewards, next_obs, dones)\n",
    "            buffer.push(transition)\n",
    "            \n",
    "            reward_this_episode += rewards\n",
    "\n",
    "            obs = next_obs\n",
    "            # save gif frame\n",
    "            if save_info:\n",
    "                frames.append(env.render())\n",
    "                tmax+=1\n",
    "        \n",
    "        if len(buffer) > batchsize and episode % episode_per_update < parallel_envs:\n",
    "            samples = buffer.sample(batchsize)\n",
    "            maddpg.update(samples, a_i, logger)\n",
    "            maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "\n",
    "        agent0_reward.append(reward_this_episode[0])\n",
    "        agent1_reward.append(reward_this_episode[1])\n",
    "        agent2_reward.append(reward_this_episode[2])\n",
    "\n",
    "        if episode % 100 == 0 or episode == number_of_episodes-1:\n",
    "            avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward), np.mean(agent2_reward)]\n",
    "            agent0_reward = []\n",
    "            agent1_reward = []\n",
    "            agent2_reward = []\n",
    "            print(np.sum(avg_rewards))\n",
    "            for a_i, avg_rew in enumerate(avg_rewards):\n",
    "                logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "\n",
    "        #saving model\n",
    "        save_dict_list =[]\n",
    "        if save_info:\n",
    "            for i in range(3):\n",
    "\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "\n",
    "                torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "                \n",
    "            # save gif files\n",
    "            imageio.mimsave(os.path.join(model_dir, 'episode-{}.gif'.format(episode)), \n",
    "                            frames, duration=.04)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnlockwood/mambaforge/envs/zoo/lib/python3.10/site-packages/gymnasium/spaces/box.py:228: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40.94948868419823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 90/1500   6% ETA:  0:00:38 |--                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.715634816823638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 195/1500  13% ETA:  0:00:41 |/////                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.62749679847067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 300/1500  20% ETA:  0:00:40 ||||||||                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.0765279245211445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 390/1500  26% ETA:  0:00:38 |----------                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.726651768673575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 495/1500  33% ETA:  0:00:35 |////////////                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519522265373972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 600/1500  40% ETA:  0:00:32 ||||||||||||||||                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5301018448142258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 690/1500  46% ETA:  0:00:29 |-----------------                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.173587141256846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 795/1500  53% ETA:  0:00:25 |////////////////////                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.302955688995741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 900/1500  60% ETA:  0:00:22 ||||||||||||||||||||||||                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.432242526052413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 990/1500  66% ETA:  0:00:18 |-------------------------              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.824786816448302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1095/1500  73% ETA:  0:00:15 |///////////////////////////           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.97584612145797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1200/1500  80% ETA:  0:00:11 |||||||||||||||||||||||||||||||        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.414472800785973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1290/1500  86% ETA:  0:00:07 |--------------------------------      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3777721861974115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1395/1500  93% ETA:  0:00:03 |///////////////////////////////////   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.669420278153481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1485/1500  99% ETA:  0:00:00 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.660149930756518\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
